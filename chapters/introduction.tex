Autonomous navigation is a fast growing area in both industry and academia. Deep learning is a promising approach to model complex functions which are structured in high dimensional data which is why it is applicable to many domains \cite{lecun2015deep}. With the recent growth of computer vision applications which utilize deep learning autonomous navigation problem has been tried to address in different approaches such as focusing on individual components \cite{kong2010general,oren1997pedestrian,bertozzi1998gold} as well as end-to-end architectures \cite{bojarski2016end, xu2016end}. This research tries to address two main problems existing in the literature which is adaptability of learning models to slightly different environments while preserving the ability to generalize to all the data and training such a system from diverse data in real world.

Regarding ground vehicles the level of autonomy there are two basic categories, namely semi-autonomy and full autonomy. Semi-autonomous vehicles are partially involved with decision making where navigation is being done with the help of human driver. Full autonomy involves vehicle being fully responsible for the whole navigation task. Autonomous navigation is being conducted with the help of several types of sensors. These include Image sensors, LIDAR and radar sensors. These sensors are mounted on self-driving cars in multiple numbers and put together to form the whole system which is complex and involves more processing power where human being driving a vehicle seems much simpler task where most of the input being only vision. Human seemingly take vision input to build the corresponding image in their minds which includes particular information relevant to the scene and task on which they try to navigate.

Regarding navigation of ground vehicles we see different road types such as muddy roads, roads under construction, roads with no lane division where vehicles driving in both directions have to share the road, roads with double lanes whose are opposite directions, highways and expressways. In each type of road, when driving a vehicle a human driver looks for different information and navigation is different. For example traveling in a highway is totally different from traveling in an expressway.  Also we see different conditions with regard to time of the day and weather. 

To cater these different scenarios coming up with a single fixed large model is inefficient. \cite{bojarski2016end, xu2016end} The number of trainable parameters will be huge and training the model with data in different contexts is difficult. although the system might be able to generalize to different contexts in each context the system might not perform as a single smaller model trained specifically to that context. The evolution of neural networks mostly contained collecting large diverse datasets \cite{deng2009imagenet,yu2018bdd100k} and to learn from those datasets making the models deeper \cite{he2016deep}. Although these approaches tried to make the learning models generalized to diverse data these approaches seem inefficient. Training a system in autonomous navigation is also a challenging task. Supervised learning involves learning from pre-annotated labels in order to generalize to the validation set which is coming from a similar or the same distribution where training data came from. Reinforcement learning involves lot of trial and errors which is impractical in driving in real world.  Also imposing a reward function is difficult unlike training agents for video games\cite{mnih2013playing}. Because the reward should include whether collision takes place. Training agent in a simulated environment does not address this issue fully\cite{SantanaH16}. The trial based approach is questionable for training an autonomous driving agent. Supervised learning approach is also not fully suitable since this approach hardly motivates the model to generalize to data from unseen distributions. 

We see much sophisticated sensors which are deployed in vehicles and robots for the purpose of navigation. Different kinds of sensors are put together to build complex systems to address the autonomous navigation problem. These systems are complex and need much computational power and energy. But a human driving a vehicle, controlling an aerial vehicle or simply walking is a less complex problem where most of the decisions are made with the help of vision input they receive. It is very important and interesting to understand how humans perform certain tasks so that we can try to mimic those behaviors in AI systems in order to make those systems more intelligent as human. 

Humans seemingly build an imaginary picture based on the input which is embedded with relevant information for a particular task(say navigation) where these images are adaptive to the type and condition of the particular task human is working on. It seems that humans take actions not basing on the pure input of their eyes but based on a feature map built in their minds. The most important evidences which highlight this facts are, people missing easily visible objects and human being unable to distinguish dreams from real scenarios. We can argue that even if the vision from eyes is not available, blind people extensively use touch from sticks and sound to build this kind of a picture in their minds. Existing navigation systems try to navigate base on the inputs received from cameras, not something which was generated, conditioned on the input\cite{xu2016end,bojarski2016end,pomerleau1989alvinn}. 

Our objective is to address the autonomous navigation problem based on conditionally generated dynamically adaptive intermediate feature maps created from pure vision input for low flying aerial vehicles and ground vehicles. In addition to that we intent to use a hybrid approach of existing learning methods to train such a system the system in real world. Also it will be interesting to conduct a study of what will these intermediate feature maps look like. Advantages include Reduce complexity of the main network. Can understand what components play a major role in vision based navigation from the input. So that we can work more towards highlighting these components to make the prediction faster and reduce the complexity of the models. 

We carry out a critical literature review with regard to computer vision, deep learning, training deep neural networks and vision based navigation in the following section. In section 3 we state our research objectives. Section 4 includes research methodology to achieve the stated objectives and section 5 includes work plan and resource requirement. Finally section 6 concludes the content of this research proposal.  
