Vision based navigation is not a recent topic but a regularly attempted problem among computer vision community. The autonomous navigation problem can be sub-categorized to three problems namely sensing the environment, localizing and mapping, and taking optimum policy to navigate. In the following subsection we review each of these areas in literature as well as end-to-end architectures for vision based autonomous navigation

\subsection{Individual Tools}

Autonomous navigation problem can be categorized into 3 basic areas which are sensing the environment, localization and mapping and optimum control policy.

Sensing the environment and understanding the scene is the first thing an autonomous vehicle should do. Applications of deep neural networks in this area involves pedestrian detection\cite{dollar2012pedestrian,oren1997pedestrian}, road sign detection\cite{maldonado2007road,fang2003road}, road/lane detection\cite{he2004color, kong2010general,bertozzi1998gold,wang2004lane,kim2017end} and segmentation \cite{long2015fully,badrinarayanan2015segnet,chen2018deeplab} Although an extensive research has already been taken place in this area, still autonomous systems are far behind than human level performance of perception learning. 

Localizing and mapping answers the question "Where am I?". Visual odometry\cite{nister2004visual} and Simultaneous Localization and Mapping(SLAM)\cite{thrun2007simultaneous, durrant2006simultaneous, bailey2006simultaneous} are problem domains which address the requirement of localizing the navigation agent in an environment and map the current position in the global map. Visual SLAM \cite{fuentes2015visual} refers to using images as the only external source in order to establish the position of the agent while reconstructing the explored zone.

Based on the intelligent information acquired from the surrounding environment and autonomous agent should come up with an optimal decision on what to do in the current context. The output of the system depends on the task system is supposed to learn which is either a specific part of navigation \cite{wang2001trajectory, rausch2017learning,bojarski2017explaining} or the full navigation problem \cite{xu2016end,bojarski2016end}

\subsection{End-to-end architectures}

ALVINN \cite{pomerleau1989alvinn} was among the first attempts to use a neural network for autonomous driving. ALVINN is a shallow neural network with only one hidden layer, designed for road following based on the raw images taken from a camera as well as a laser range finder. The system was trained using back-propagation\cite{rumelhart1986learning} and in order to make the system adaptive to varying conditions authors have used a simulated road generator which provided more road images. This simple system was surprisingly performing well and inspired the subsequent research. 

Among recent developments Bojarski et al\cite{bojarski2016end} proposed a raw pixel to steering output system with the use of CNNs. This work by NVIDIA was somewhat similar to ALVINN where the raw images were from a single front-facing camera mounted on a vehicle. The algorithms are fairly simple and the cars were able to navigate in simple environments such as highway, following lanes and obstacle free roads. Xu et al \cite{xu2016end} proposed an approach to learning a generic model from large scale diverse video data, training an end-to-end architecture for autonomous driving. Authors have deployed a FCN-LSTM architecture and used scene segmentation as a side task. The model addresses autonomous driving as a future ego motion prediction problem and were able to formulate as a generic model. Santana \& Hotz\cite{SantanaH16} also addressed the autonomous navigation problem as a visual prediction task where future video frames are predicted using previous frames. The system was trained in a simulator and the learning approach is a combination of GANs and Variational Auto Encoders\cite{kingma2013auto}. We also can observe end-to-end architectures focused on limited contexts such as highway driving\cite{huval2015empirical},lane keeping\cite{chen2017end} and steering\cite{bojarski2017explaining}. 

Training these models in real world is challenging. We can observe that this issue has been tried to address in several ways. One is training the system in a simulated environment\cite{SantanaH16,abbeel2004apprenticeship,lillicrap2015continuous}. But still there exists a huge gap between a modeled environment and real world. Most of the existing end-to-end architectures have been trained in large scale datasets in a supervised manner\cite{pomerleau1989alvinn,bojarski2016end,xu2016end}. The prediction based driving model proposed by Santana \& Hotz\cite{SantanaH16} is a combination of generative models which shows more realistic in terms of training in real world.

All the above discussed approaches do not fully solve the problem of an adaptive single architecture for autonomous driving. Researchers have tried to gather large scale diverse visual data either by fully recording \cite{xu2016end} \cite{yu2018bdd100k} or generating varying samples from the original distribution. Trying to train a generic model with fixed parameters is inefficient because a much simpler model trained only in one context might outperform the large generic model in that particular context. The intelligence of switching between similar models or varying the content of the model is not yet fully addressed in the literature. Also training such a system in real world is still a challenging task where supervised learning involves lot of data and making models memorize while reinforcement learning involves trial based approach which is impractical for an autonomous driving agent. Training an autonomous agent in a simulated environments\cite{SantanaH16,abbeel2004apprenticeship,lillicrap2015continuous} is hardly a practical solution since there exists a considerable gap between real world and simulated environments. In fact modeling the varying contexts in real world navigation in a simulated environment is itself an impossible task.
