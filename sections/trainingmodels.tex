\subsection{Learning Algorithms}
\label{se:supervisedlearning}

In literature we observe three main types of learning algorithms to train a neural network which are supervised learning, unsupervised learning and reinforcement learning. Here we intend to discuss in detail about supervised and reinforcement learning as those two are the most commonly used approaches to learning of neural networks associated with computer vision.

In supervised learning a model is given a dataset containing inputs and the associated outputs which are namely labels. Models are supposed to learn from these data and 

Reinforcement learning
\cite{mnih2013playing} playing atari Mnih et al

Since the introduction of generative adversarial networks (GAN)\cite{goodfellow2014generative} extensive research has already been taken place with regard to GAN based image inpainting. The deep convolutional generative adversarial networks (DCGAN) introduced in Radford et al.\cite{radford2015unsupervised} which proposed several improvements to the architectural topology of convolutional GAN which stabilized the training of GANs. This work inspired subsequent research on GANs which involved deep convolutional networks. Conventional GANs are using merely noise as input to the generator and the goal is to learn a distriution. Conditional GANs adds informative conditioning variable in addition to noise which lets the model to learn a conditional distribution. Conditional GANs have been deployed in generating images conditioned on class label\cite{mirza2014conditional}, conditioned on text\cite{reed2016generative}, conditioned on image\cite{isola2017image} etc.

Varitaional autoencoders \cite{kingma2013auto}


\subsection{Training a network}
\label{se:rl}

Training a deep neural network is a challenging task merely due to the large number of parameters. Apart from the learning algorithm used in a deep neural network, adjusting the weights of connections in a network is also an important task. Despite of the learning algorithm most widely used approach to train the weights of a neural network is back-propagation introduced by rumelhart et al \cite{rumelhart1986learning}. Back-propagation repeatedly adjusts the weights of a neural network in order to minimize the distance between the actual output and the expected output of the network. 

\subsection{Unsupervised Training}
\label{se:adversariatraining}

