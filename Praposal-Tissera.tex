\documentclass[a4paper,oneside,12pt]{report}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{cite}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{refstyle}
\usepackage[titletoc]{appendix}

%\usepackage{tocloft}
%\usepackage[Lenny]{fncychap}

\usepackage[a4paper,bindingoffset=0.0mm,%
left=40mm,right=25mm,top=25mm,bottom=40mm,%
footskip=.25in]{geometry}
\usepackage{blindtext}


\renewcommand\contentsname{Table of Contents}

%\renewcommand{\cftchappresnum}{Chapter }  
%\cftsetindents{chapter}{0em}{6em}  
%\addtocontents{toc}{~\hfill\textbf{Page}\par}       % add 'Page' to ToC


\renewcommand{\baselinestretch}{1} 

\titleformat{\chapter}[display]
{\filleft\normalfont}{\scshape\chaptertitlename\ \thechapter}{-20pt}{\large\bfseries\MakeUppercase}
[\vspace{-3.5ex}
\rule{\textwidth}{0.3pt}]
\titlespacing{\chapter}
{0pt}{10pt}{20pt}
\renewcommand{\chaptername}{Section}


\titleformat{\section}[hang]
{\normalfont\bfseries}{\thesection}{0.5em}{}
\titlespacing{\section}
{0pt}{20pt}{20pt}

\titleformat{\subsection}[hang]
{\normalfont\bfseries}{\thesubsection}{0.5em}{}
\titlespacing{\subsection}
{0pt}{10pt}{10pt}

\titleformat{\subsubsection}[hang]
{\normalfont\bfseries}{\thesubsubsection}{0.5em}{}
\titlespacing{\subsubsection}
{0pt}{10pt}{10pt}


\begin{document}
\begin{titlepage}
	\begin{center}
		\vspace*{5cm}
		
		\huge
		\textbf{Vision Based Navigation for Aerial and Ground Vehicles }
		
		\vspace{0.5cm}
		
		\large
		\vspace{0.5cm}
		
		M.H.G.D. Tissera
		
		\vspace{0.5cm}
		
		(188013F)
		\vfill
		
		\normalsize
		Research Proposal 
		
		\vspace{0.8cm}
		\large
		
		Department Electronic and Telecommunication Engineering\\
		\vspace{0.8cm}
		University of Moratuwa\\
		Sri Lanka\\
		\vspace{0.8cm}
		June 2018
		
	\end{center}
\end{titlepage}

\pagenumbering{roman}

\setlength{\parskip}{1em}
\setstretch{1.5}

% Declaration

%\chapter*{Declaration}
%\addcontentsline{toc}{chapter}{Declaration}
\begin{center}
\huge
\textbf{UNIVERSITY OF MORATUWA}
\end{center}

\begin{flushleft}
\large
\textbf{Declaration}
\end{flushleft}
\setlength{\parindent}{0cm}

I declare that this is my own research proposal and this proposal does not incorporate without acknowledgment any material previously published submitted for a Degree or Diploma in any other university or institute of higher learning and to the best of my knowledge and belief it does not contain any material previously published or written by another person except where the acknowledgment is made in the text.

Signature:  \hspace{7cm}  Date:\\
\\
................................. \hspace{5cm} ....................\\
M.H.G.D. Tissera

I have read the proposal and it is in accordance with the approved university proposal outline. I am willing to supervise the research work of the above candidate on the proposed area.

Signature of the Supervisor(s):  \hspace{3.25cm}  Date:\\
\\
.................................... \hspace{4.25cm} ....................\\
Dr.Ranga Rodrigo\\
\\
.................................... \hspace{4.25cm} ....................\\
Dr.Beshan Kulapala\\
\\


\clearpage


% abstract
\begin{flushleft}
	\large
	\textbf{Abstract}
\end{flushleft}
\addcontentsline{toc}{chapter}{Abstract}
\setstretch{1.5}
\small

To be filled 

\cite{R01} \cite{R02} \cite{R03} \cite{R04} \cite{R05} \cite{R06} \cite{R07} \cite{R08} \cite{R09} \cite{R10} \cite{R11} \cite{R12} \cite{R13} \cite{R14} \cite{R15} \cite{R16} \cite{R17} \cite{R18} \cite{R19} \cite{R20} \cite{R21} \cite{R22} \cite{R23} \cite{R24} \cite{R25} \cite{R26} \cite{R27} \cite{R28} \cite{R29} \cite{R30} \cite{R31} \cite{R32} \cite{R33} \cite{R34} \cite{R35} \cite{R36} \cite{R37} \cite{R38} \cite{R39} \cite{R40} \cite{R41} \cite{R42} \cite{R43} \cite{R44} \cite{R45} \cite{R46} \cite{R47} \cite{R48}
\cite{R49} \cite{R50} \cite{R51} \cite{R52} \cite{R53} \cite{R54} \cite{R55} \cite{R56} \cite{R57} \cite{R58} \cite{R59} \cite{R60}
\cite{R61} \cite{R62} \cite{R63} \cite{R64} \cite{R65} \cite{R66}


\textbf{\textit{Keywords:} XXXXXXXXXXXX}



\setlength{\parindent}{5mm}
\normalsize
\setstretch{0.8}

\tableofcontents
\addcontentsline{toc}{chapter}{Table of Contents}
\clearpage


\clearpage

\pagenumbering{arabic}

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\setlength{\parindent}{5mm}
\normalsize
\setstretch{1.5}
\chapter{Introduction}
\label{ch:introduction}



% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\chapter{Literature Review}
\label{ch:litearure review}

\section{Autonomous Navigation Architectures}


\section{Reinforcement Learning for Autonomous Navigation}

\subsection{Reinforcement Learning Framework}

%\begin{figure}[h]
%\begin{center}
%  \includegraphics[scale = 0.5]{Figure01.png}
%  \caption{Reinforcement Learning Faramework.}
%  \label{fig:Figure01}
%\end{center}
%\end{figure}

\subsection{Reinforcement Learning in the Context of Robotics}


\subsection{Deep Reinforcement Learning}




 
\subsection{Reinforcement Learning with Continuous state and action spaces }




The advancement in fields of Deep Learning (DL) and Reinforcement Learning (RL) has dramatically improved the perforce of DQN algorithms while overcoming above described ingrained weaknesses found in deep Q-network. Thimothy \textit{et al.} \cite{R06} has proposed a model-free, off-policy actor-critic algorithm termed as Deep Deterministic Policy Gradient (DDPG) algorithm (see the appendix B), by employing deep neural network as function appropriator that can learn competitive policies in high-dimensional continuous action space. Their approach is based on Deterministic Policy Gradient (DPG) algorithm \cite{R14} and it is a combination of actor-critic approach\cite{R55} and outstanding features of DQN \cite{R04} \cite{R06}. 



\subsection{XXXXXXXXX}

To be filled


\section{Knowledge Transfer Methods in Robotics}

\subsection{XXXXXXXXXXX}

To be filled

\subsection{XXXXXXXXXXXX}

To be filled
% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\chapter{Research Objectives}
\label{ch:research objectives}

The main objective of this research is to develop a common architecture which facilitate to learn the behavior of various robotics platforms to overcome the platform dependency problem and incorporate it for autonomous navigation of various robotics platforms. 

Research Objectives:

\begin{enumerate}
%1	
\item To Develop a common architecture to overcome the platform dependency problem when performing high-level navigation tasks

%2
\item To create an agent which is operating on the proposed architecture with the capable of learning the platform behavior 

%3
\item To utilize the developed architecture for autonomous navigation of different robotics platforms 

%5
\item To develop and implement efficient, effective and safe methods to train the agent as appropriate with autonomous navigation.  

%6
\item To evaluate the performance of proposed architecture by training the agent to perform some high-level navigation task and transferring the knowledge gained by one platform to a similar kind of another platform 




\end{enumerate}

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\chapter{Research Methodology}
%\vspace{-5mm}
\label{ch:research methodology}

\begin{enumerate}
%1	
\item Carrying out comprehensive literature review as covering following areas to develop a suitable architecture for autonomous navigation of mobile robots.
	\begin{itemize}
		\item Conventional autonomous navigation methods for mobile robot
		\item Artificial Intelligence (AI)
		\item Artificial Neural Networks (ANN)\cite{R09}
		\item Cognitive architectures
		\item Memory architectures for autonomous navigation
		\item Deep Reinforcement Learning (DRL) in robotics 
		\item Knowledge transfer methods in robotics

	\end{itemize}
	
%2
\item Implementing an adaptable Deep Reinforcement Learning (DRL) based virtual robot agent which is capable of performing different navigation tasks such as path planing,target reaching and obstacle avoidance \textit{etc}, and simulating it in virtual Robotic Experiment Platform (V-rep)\cite{R60}. The purpose of implementing this agent is to get a better understanding of training based navigation problems and to study about various training methods.

%3
\item Studying about Knowledge transferring and incorporating the existing knowledge to develop efficient and safe training methods.

%4
\item Studying about existing cognitive architectures and developing a common architecture to overcome the platform dependency problem.

%5
\item Developing an agent to run on the developed architecture and evaluate the performance of it using two quad-rotor platforms.

%6
\item The successful operation of proposed architecture will be demonstrated using two quad-rotor platforms with a vision system. One of the quad-rotor will be controlled manually and that will be used to train the second quad-rotor. The second quad-rotor should be able to learn the platform behavior by itself and follow the movement of the manually controlled quad-rotor

%7
\item To demonstrate the platform independent learning ability, some of the controlled parameters of second quad rotor will be changed after the first training and the ability of mimic the movement of  manually controlled quad-rotor with the same performance, will be evaluated.



\end{enumerate}
% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\chapter{Work Plan and Resource Requirements}
\label{ch:work plan and resource requirements}

% Image of work plan
%\begin{figure}[h]
%\begin{center}
  %\includegraphics[scale = 0.45]{work_plan.png}\\
  %\caption{Work Plan.}\label{fi:work_plan}
%\end{center}
%\end{figure}

Resource Requirement

\begin{enumerate}
\item Two quad-rotors with a vision system and remote control capability.
\item Two high performance single board computers

\end{enumerate}

% --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\chapter{Conclusion}
\label{ch:conlusion}

To be filled 

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------

\renewcommand{\bibname}{\normalfont\selectfont\normalsize References}
\bibliographystyle{IEEEtran}
\bibliography{reference}
\addcontentsline{toc}{chapter}{References}
\renewcommand{\bibname}{whatever}

\begin{appendices}
\chapter{Deep Q-Learning Algorithm with Experience Replay}

Deep Q-Learning algorithm with experience replay technique as proposed by Mnih \textit{et al.} in 2015.

\chapter{Deep Deterministic Policy Gradient Algorithm}

Deep Deterministic Policy Gradient (DDPG) Algorithm proposed by Thimothy \textit{et al.} in 2016.

\chapter{Asynchronous One-Step Q-Learning Algorithm}

\chapter{Asynchronous n-Step Q-Learning Algorithm}

\chapter{Asynchronous Advantage Actor-Critic Algorithm}

\end{appendices}

\end{document}
